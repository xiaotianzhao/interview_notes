第十章 序列建模：循环和递归网络

从多层网络到循环网络的原因：在模型的不同部分共享参数。参数共享使得模型能够扩展到不同形式的样本。

“I went to Nepal in 2009” , "In 2009, i went to Nepal" 任务是抽取陈述者去Nepal的时间，无论其实在第六个还是第二个位置。

传统的前馈神经网会给每个输入特征分配一个单独的参数，所以需要分别学习句子的每个位置的所有语言规则。循环神经网络在几个时间步内共享相同的权重，不需要分别学习句子在每个位置的所有语言规则。

一个相关的想法是一维上面的卷积。参数共享体现在共用一个卷积核。

**展开计算图**

当训练循环网络根据过去预测未来的时候，网络一般要学会使用$h^{t}$ 作为过去序列的与任务相关的**有损摘要** 。根据不同的训练准则，摘要可能选择性的精确保留过去序列的某些方面。比如，LM之中，没必要保存前面序列的全部信息，保留能够预测下一个单词的信息就好。最严苛的是自编码器架构，要求$h^t$ 保存的信息足够丰富，能够大致恢复序列。

展开过程的两个优势：

- 无论序列长度，学习的模型始终具有相同的输入大小，因为它指定的是从一个状态到另一个状态的转移，而不是在变长的历史状态上面的操作。
- 可以在每个时间步使用相同参数的转移函数$f$

学习单一共享模型，可以很容易泛化到没有见过的长度（没有在训练数据中出现），并且由于参数较少，所需要的训练样本远远小于不带参数共享的模型。



**循环神经网**

循环神经网络的一些设计模式：

- 在每个时间步都有输出，并且隐藏单元之间有循环连接的循环网络。
- 在每个时间步都有输出，只有当前时刻的输出到下个时刻的隐藏单元之间有循环连接的输出
- 隐藏单元之间存在循环连接，但是读取整个序列后产生单个输出的循环网络。

RNN计算公式：

RNN一般的实现中，这个就是全部了

$a_t = b + W_a[h_{t -1},x_t]$

$h_t = tanh(a_t)$

后面这两个公式一般是自己实现输出层

$o_t = c + W_oh_t$

$\hat{y_t} = softmax(o_t)$



**导师驱动过程和输出循环网络**

仅在一个时间步的输出和下一个时间步的隐藏单元之间存在连接的循环连接的网络不是很强大。

因为网络缺少隐藏到隐藏的循环，它要求输出单元捕捉用于预测未来的关于过去的所有信息。但是输出单元又被训练成匹配训练集的目标，他们不太能捕获关于过去输入历史的必要信息。除非用户知如何描述系统的全部状态，并将它作为训练目标的一部分。



消除隐藏到隐藏循环的优点在于：任何基于比较时刻$t$的预测和时刻$t$的训练目标的损失函数的所有时间步都解耦。

由输出反馈到模型而产生循环连接的模型可以用**导师驱动过程** 。在训练模型时，导师驱动过程不再使用最大似然准则，而在时刻$t+1$接收$t$时刻的真实值作为输入。 **条件最大似然准则** ：

$log\ p(y_1,y_2\mid x_1, x_2) = log\ p(y_2 | y_1, x_1,x_2) + log \ p(y_1| x_1,y_1)$

在时刻$t =2$的时候，模型被训练为最大化$y_2$的条件概率。因此最大似然在训练时指定正确反馈，而不是将自己的输出反馈到模型。

如果之后的网络在开环模式下使用，即网络输出反馈作为输入，那么完全使用导师驱动过程进行训练的缺点就会出现。在训练期间和测试期间看到的输入会有很大的不同。两种解决方案：

- 同时使用导师驱动过程和自由运行输入过程进行训练
- 通过随意选择生成值或真实的数据值作为输入以减小训练时和测试时看到的输入之间的差别。使用了课程学习的策略，逐步使用更多生成值作为输入。



**计算循环神经网络的梯度**

$softmax(x) = (\frac{e^{x_i}}{\sum_{i}^{n}e^{x_i}})$

$softmax(x)的导数 ： e^{x_i}(1 - e^{x_i}) $

![RNN计算图](.\image\RNN计算图.png)



![BPTT](.\image\BPTT.png)



作为有向图模型的循环网络

均方误差是与单位高斯分布的输出相关联的交叉熵损失。

RNN看做是有向图模型：

![RNN_AS_DG](.\image\RNN_AS_DG.png)

在图模型中加入隐藏单元可以对观测的联合分布提供非常有效的参数化。在这个图模型中，遥远过去的变量$y^i$ 通过对$h$的影响来影响变量$y^t$。该图的结构说明可以在时间步使用相同的条件概率分布有效的参数化模型。

![RNN_AS_DG_2](C:\Users\xtzhao\Desktop\GitProjects\interview_notes\image\RNN_AS_DG_2.png)



循环网络为减少参数数目付出的代价是优化参数可能变得困难。在循环网络中使用参数共享的前提是**相同的参数可用于不同时间步的假设，也就是说和时刻$t$是无关的**。

RNN必须有某种机制来确定序列的长度。

- 当输出是一个词汇表的情况下，添加一个对应于序列末端的特殊符号。产生该符号，采样停止
- 在模型中引入一个伯努利分布，表示在每个时间步继续生成还是停止。比如说sigmoid函数
- 将一个额外的输出添加到模型并预测整数$T$本身 。并在每一步加一个输入$T$ 或者$T-t$ 。



**基于上下文的RNN序列建模**

RNN允许将图模型的观点扩展到给定$x$后$y$条件分布。$P(y \mid \omega)$ 代表分布$P(y \mid x)$ ，其中$\omega$ 是关于$x$的函数。可以通过不同的方式来实现。

- 使用$t = 1,...,\tau$ 的向量$x^{t}$ 作为输入的RNN。
- 使用单个向量$x$作为输入。

将一个额外的输入提供到RNN中的一些常见方法：

- 在每一个时刻作为一个额外的输入：
  - 对于使用$x$的情况， 如果只是做了$concat(x, y^t)$操作的话，可以看做加了一个以输入$x$ 为自变量的偏置参数，这种模型采取了一个非条件模型的$\theta$ 。
  - 对于接受向量序列$x^t$ 作为输入，而不是仅仅接受单个向量$x$的情况。 $P(y^1, ..., y^{\tau} \mid x^1, ..., x^{\tau}) = \prod_{t} P(y^t \mid x^1, ... ,x^t)$  为了去掉条件独立的假设，可以在时刻$t$ 输出到$t+1$ 的隐藏单元添加连接。
- 作为初始状态$h^0$ 
- 结合两种方式



**双向RNN**

动机：**在许多应用中，我们要输出的$y^{t}$ 预测可能依赖于整个输入序列**。

在语音识别中，当前声音作为音素的正确解释可能取决于未来几个音素，甚至潜在的可能取决于未来的几个词语， 因为词与词附近的词之间可能存在语义依赖。

双向RNN允许输出单元$o^t$ 能够计算同时依赖于过去和未来且对时刻$t$的输出值最敏感的表示，而不必指定t周围固定大小的窗口。

双向RNN可以扩展到二维输入，上下左右四个方向的RNN。$o^{i,j}$ 可以捕获到全局的信息。



**基于编码-解码的序列到序列框架(Seq2Seq)**

动机：如何训练RNN，使其将输入序列映射到一个不一定等长的输出序列。应用场景：语音识别，机器翻译或者问答，其中训练集的输入和输出的序列长度通常不相同。

编码器：处理输入序列，输出上下文信息C(通常是最终隐藏状态的简单函数)。解码器：以固定长度的向量为条件产生输出序列$Y=(y^1, y^2, ..., y^{n_y)})$

上下文信息$C$ 可以以两种方式输入到RNN。 作为初始状态，或者在每个时间步都进行输入。

Attention提出的动机：上下文信息C的维度太小，不足以概括一个长序列。



**深度循环网络**

大多数RNN可以分解为三块参数及其相关变化：

- 从输入到隐状态的变化
- 从前一隐藏状态到下一隐藏状态的变化
- 从隐藏状态到输出

可以认为层次结构中较低的层起到了将原始输入转化为对更高层的隐藏状态更合适表示的作用。

多层的循环神经网络的多种实现方式：

- 隐藏循环状态可以被分解为具有层次的组
- 可以向输入到隐藏，隐藏到隐藏，隐藏到输出部分引入更深的计算（如MLP），这样可以延长连接不同时间步的最短路径
- 可以引入跳跃连接来缓解路径的延长效应



**递归神经网络**

递归神经网络代表循环网络的另一个扩展，它被构造深的树状结构而不是RNN的链状结构，因此是不同类型的计算图。递归网络的一个优势在于，对于具有相同长度$\tau$ 的序列，深度可以急剧地从$\tau$ 降低到$log \ \tau$ ，这可能有助于解决长时依赖问题。

如何以最佳的方式构造树？

- 不依赖于数据的树结构，如平衡二叉树
- 一些领域内，外部方法可以为选择适当的树结构提供借鉴。如NLP的语法分析树。



**长时依赖的挑战**

长时依赖的根本问题：经过许多阶段传播后的梯度倾向于消失（大部分情况）或爆炸（很少，但是对优化过程影响很大）。即使假设神经网络是参数稳定的（可存储记忆，且梯度不发生爆炸），但长期依赖的困难来自比短期相互作用**指数小的权重**。

重复组合函数。当组合许多非线性函数（如RNN中的tanh函数），结果是高度非线性的，通常大多数值与微小的导数相关联，也有一些大导数的值，以及在增加和减小之间多次交替。

循环神经网络所使用的函数组合有点儿像矩阵乘法。我们可以认为，循环联系：

$h^t = W^Th^{t -1}$ 

$h^t = (W^t)^T h^0$

$W^t = Q\Lambda Q^T$

$h^t = Q \Lambda^tQ^Th^0$

特征值提升到$t$次之后，导致振幅不到1的特征值衰减到0，而振幅大于一的也正值就会激增。任何不与最大特征向量对齐的$h^0$ 的部分最终将被丢弃。

**长时依赖问题是针对循环网络的**。在标量情况下，多乘一个权重$w$ 。该乘积$w^t$ 是爆炸还是消失取决于$w$的幅值。如果每个时刻不同的权重$w^t$ 的非循环网络，情况就不同了。 如果初始状态给定为1，那么时刻$t$ 的状态可以由$\prod_{i=1}^{t} w^i$ 给出 。假设$w^i$ 的值是随机生成的，各自独立，且均值为0，方差为v。乘积的方差就为$O(v^n)$ 。为了获得某些期望的方差$v^*$ ，可以选择单个方差$v = {}^{n}\sqrt{v^*}$

因此，非常深的前馈网络通过精心设计的比例可以避免梯度消失和爆炸问题。



**回声状态网络**

从$h^{t -1}$ 到$h^t$ 的循环权重映射以及从$x^t$ 到$h^t$ 的输入权重映射是循环神经网络中最难学习的参数。

解决这种困难的方法是**设定循环隐藏单元**  ，使其能够很好的捕捉过去的信息，并且**只学习输出权重**。

现在回声状态网络的最近工作提倡使用远大于1的谱半径。

- 回声网络的策略是简单的固定权重使其具有一定的谱半径如3，其中信息通过时间向前传播，但是会由于饱和的非线性单元的稳定作用而不会爆炸
- 回声网络最初的想法是使状态到状态的Jacobian矩阵的特征值接近于1。



**渗漏单元和其他多时间尺度的策略**

处理长时依赖的一种方法是设计工作在多个时间尺度的模型，使模型的某些部分在细粒度的时间尺度上操作并能处理小细节，而其他部分在粗时间尺度上操作并能把遥远过去的信息更有效的传递过来。

**构建粗细时间尺度的策略** 

- 在时间轴上面增加跳跃连接
- “渗漏单元”使用不同的事件常数整合信号，并去除一些用于建模细粒度时间尺度的连接

**时间维度的跳跃连接**

增加从遥远过去的变量到目前变量的直接连接是得到出时间尺度的一种方法。梯度关于时间步呈指数消失或者爆炸。引入$d$ 时延的循环连接以减轻这个问题。现在导数指数减小的速度与$\frac{\tau}{d}$ 而不是$\tau$ 。但是仍然可能成$t$指数的爆炸。这允许学习算法捕获更长的依赖性。但是并不是所有的长时依赖都可以在这种方式下良好的表示。

**渗漏单元和一系列不同的时间尺度**

获取导数乘积接近1的另一种方式是设置线性自连接单元，并且这些连接的权重接近1。

使用计算得到的某些$v$值，应用更新$\mu^t \gets \alpha \mu^{t-1} + (1 - \alpha) v$ 。当$\alpha$ 接近1的时候，滑动平均值能记住过去很长一段时间的信息，当$\alpha$ 接近0的时候，关于过去的信息被迅速丢弃。

d时间步跳跃连接可以确保单元总能被d个时间步前的那个值影响。使用权值接近1的线性自连接是确保该单元可以访问过去值的不同方式。线性自连接通过调节实值$\alpha$ 更平滑的调整这种效果。



**删除连接**

处理长时依赖的另一种方法是在多个时间尺度上组织RNN状态的想法，信息在较慢的事件尺度上面更容易长距离流动。

这个想法与时间维度上面的跳跃连接不相同，因为它涉及主动删除长度为一的连接并使用更长的连接替换他们。而跳跃连接，是在添加边。收到这种新连接的单元，可以学习在长时间尺度上面运作，也可以选择专注在自己其他的短期连接上面。



**长短期记忆和其他门控RNN**

门控单元的想法：基于生成通过时间的路径，其中导数既不消失也不发生爆炸。**希望神经网络学会什么时候积累信息，什么时候清除状态，而不是手动决定，这就是门控RNN要做的事情**。

引入自循环的思想，以产生长时间持续流动的路径是初始长短期记忆的贡献，其中一个关键扩展是使自循环权重视上下文而定。

GRU与LSTM的主要区别是，单个门控单元同时控制遗忘因子和更新状态单元决定。

LSTM中的关键因素是遗忘门，向遗忘门中加入1的偏置能够让LSTM变得跟已探索的最佳变种一样健壮



**优化长期依赖**

强非线性函数往往倾向于非常大或非常小幅度的梯度。易于产生悬崖，宽且相当平坦区域被目标函数变化快的小区域隔开，形成悬崖。

这导致的困难在于，当参数的梯度非常大的时候，梯度下降的参数更新可能将参数抛出很远，进入目标函数较大的区域，到达当前解所做的努力就变成了无用功。

梯度截断两种方式：

- 在参数更新之前，逐元素的截断小批量产生的参数梯度（超过阈值，赋值为阈值）
- 在参数更新之前截断梯度$g$ 的范数 $if \mid \mid g \mid \mid \gt v \; \; g = \frac{v}{\mid\mid g\mid \mid} g$

后一种方法优点在于保证了每个步骤仍然是在梯度方向上。



**引导信息流的正则化**

一个想法是：在展开循环架构的计算图中，沿着与弧边相关联的梯度乘积接近1的部分创建路径。实现这一点的方法是使用LSTM或者自循环和门控机制。

另一个想法：正则化或约束参数，以引导信息流。特别是即使损失函数只对序列尾部的输出做惩罚，我们也希望梯度向量在反向传播的时候能够保持其幅度。

形式上要求：$\frac{\partial L}{\partial h^t} \frac{\partial h^t}{\partial h^{t-1}}$ 与$\frac{\partial L}{\partial h^t}$ 一样大。

在这个目标下，$\Omega = \sum_{t} (\frac{\mid\mid \frac{\partial L}{\partial h^t} \frac{\partial h^t}{\partial h^{t-1}}  \mid\mid}{\mid\mid \frac{\partial L}{\partial h^t} \mid\mid} - 1)$

计算这一梯度的正则项可能会出现困难，可以考虑使用恒值来作为$\frac{\partial L}{\partial h^t}$

实验表明：使用该正则项与启发式截断相结合，该正则项可以有效的增加RNN可以学习的依赖长度。

弱点：在处理数据冗余的任务时如语言模型，它并不像LSTM一样有效。



**外显记忆**

神经网络擅长存储隐性知识，但是很难记住事实。推测这是因为神经网络缺乏**工作存储**系统，即类似人类为实现一些目标而明确保存和操作相关片段的系统。

记忆网络（Memory Network），神经图灵机（Neural Turning Network）。

