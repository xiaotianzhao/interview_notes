为什么说Bagging是减少variance而Boosting是减少bias?

Bagging对样本重采样，对每一重采样得到的子样本集训练一个模型，最后取平均。由于子样本集的相似性以及使用的是同种模型，因此各模型有近似相等的bias和variance（事实上，各模型的分布也近似相同，但不独立）。

$E(F) = E(\sum_{i}^{m} \lambda_i * f_i) = \sum_{i}^{m} \lambda_iE(f_i) $

$var(F) = var(\sum_i^{m}\lambda_i f_i) = cov(\sum_i^{m}\lambda_i f_i , \sum_i^{m}\lambda_i f_i)=\sum_i^{m}\sum_i^{m}\lambda_i^2var(f_i) * \rho + \sum_i^{m} \sum_{j \ne i}^{m} 2 *\lambda_i * \lambda_j * \sqrt{var(f_i)}\sqrt{var(f_j)}* (1 - \rho) $

对于Bagging 来说，每个模型权重为1/m且期望近似相等：

$E(F) = \sum_i^m \frac{1}{m} \mu = \frac{1}{m} * m * \mu$

$var(F) = m^2 * \frac{1}{m^2} * \sigma^2 * \rho + m * \frac{1}{m^2} * \sigma ^2 *(1- \rho) = \sigma^2 * \rho + \sigma^2 * (1 - \rho) * \frac{1}{m}$

整体模型的期望近似于基模型的期望。

整体模型的方差小于等于基模型的方差，当相关性取到1的时候相等。随着m的增大，整体模型的方差减少。bagging需要使用**强模型**，否则会导致整体模型偏差度较低，即准确度较低。



对于boosting来说，基模型的训练集抽样是强相关的，那么模型的相近系数近似等于1。

$E(F) = \sum_i^{m} \lambda_i E(f_i)$

$Var(F) = m^2 * \lambda^2 * \sigma^2 * \rho + m * \lambda^2 * \sigma^2 * (1 - \rho) = m^2 * \lambda^2 * \sigma^2$

对于偏差：因为基模型为弱模型，导致了每个基模型的准确度都不是很高（因为其在训练集上的准确度不高）。随着基模型数的增多，后续模型的准确度提高，且准确度高的模型权重较大，整体模型的期望值增加，更接近真实值，因此，整体模型的准确度提高。会接近1么？模型准确度上来之后，方差变大，防止 过拟合的能力降低。

boosting需要使用的是**弱模型**，否则方差过大。



 多分类学习：

OvO:

一共需要$N*(N-1) /2$个二分类模型，在预测的时候，使用$N*(N-1) / 2$个模型进行预测，选择出现次数最多的类别作为预测结果。

OvM：

一共需要N个模型二分类模型（选择一个模型作为正例，其余作为负例），在预测时候，如果只有一个预测为正例，其余预测为负例，那么选择预测为正例的分类器作为预测结果。若有多个分类器得到正分类结果，则根据置信度选择一个最有可能的。

MvM:

![ecoc](.\image\ecoc.png)

纠错输出码(ECOC)。可以采用三元编码，1表示正例，0表示不使用该样本，-1表现负例。对于同一个学习任务，编码越长，纠错能力越强。



不均衡类别问题：

类别不均衡是指分类任务中不同类别的训练样例数目差别很大的情况。

三种解决方案：

- 再缩放

  对于类别均衡的分类器，$\frac{y}{1-y} \gt 1$ ，其中$y$表示正确类别的可能性，且$y \in [0,1)$ ，对于不均衡的样本，可以做$\frac{y}{1- y} \gt \frac{m_{+}}{m_{-}}$ 。更换一个形式为，$\frac{y}{1-y} * \frac{m_{-}}{m_{+}} \gt \frac{m_{+}}{m_{-}} * \frac{m_{-}}{m_{+}} = 1$ 

  再缩放的要求训练集是真实样本的无偏采样，这个假设往往不成立，也就是说，未必能有效的基于观测几率来推断出真实几率。

- 下采样

  对较多的类别进行欠采样，即除去较多的类别使其和较少的类别数据大致相等。

- 上采样

  对较少的类别进行过采样，即对较少的样本重复采样。一般不会进行简单的重复采样，有一种是对正例进行插值产生新的样本。



PCA:

输入：$X_{n*m}$ 输入样本, n是样本的数目，m是样本的维度

输出：$X_{n*k}$ n个样本，k个维度的输出

1. 对 $X$ 进行中心化处理 $x_i = x_i - \frac{1}{n} \sum_{j}^{n} x_j$
2. 计算$X_TX$
3. 计算$(X_TX)_{m*m}$的特征值和特征向量
4. 对特征向量排序，选择最大的k个特征值和其对应的特征向量，将特征向量组成一个$m*k$的矩阵$W_{m*k}$。
5. 计算$X_{n*m} W_{m*k}$ ,返回得到的值



PCA的基本思想，对所有样本做投影，使得投影的方差最大



感知机的损失函数：

$\sum_{x_i\in M} -y_i(wx_i+b)$ 其中M是误分类集合



感知机收敛性的证明：



